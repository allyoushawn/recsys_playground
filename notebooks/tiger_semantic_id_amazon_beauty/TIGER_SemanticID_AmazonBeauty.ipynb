{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TIGER SemanticID on Amazon Beauty — Experiment Plan\n",
        "\n",
        "Goal: Implement Semantic IDs via RQ-VAE and a compact seq2seq Transformer for generative retrieval on Amazon Beauty 5-core; produce metrics and visualizations validating paper claims.\n",
        "\n",
        "Datasets: Amazon Product Reviews (Beauty, 5-core).\n",
        "\n",
        "Key steps: Download & preprocess; Sentence-T5 embeddings; RQ-VAE (3 levels, K=256) to 3-tuple codes + collision code c4; visualizations (c1↔category, hierarchy); seq2seq generative retrieval; metrics Recall@5/10, NDCG@5/10 and invalid-ID rate; ablations (Random/LSH); mini cold-start probe.\n",
        "\n",
        "Artifacts: save to /content/artifacts. Keep configs modest for Colab; add knobs for smoke tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61ec680b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo, install dependencies, and make src importable (Colab-friendly)\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "repo_url = 'https://github.com/allyoushawn/recsys_playground.git'\n",
        "repo_dir = 'recsys_playground'\n",
        "branch_name = '20250908_tiger_dev'\n",
        "\n",
        "import os\n",
        "if IN_COLAB:\n",
        "    if not os.path.exists(repo_dir):\n",
        "        !git clone $repo_url\n",
        "    %cd $repo_dir\n",
        "    !git fetch --all\n",
        "    !git checkout $branch_name || echo 'Branch not found; staying on default.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Runtime & installs\n",
        "import os, sys, subprocess, torch\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# Install module dependencies (Colab).\n",
        "!pip -q install -r tiger_semantic_id_amazon_beauty/requirements.txt\n",
        "\n",
        "# Make src importable\n",
        "src_path = os.path.abspath('tiger_semantic_id_amazon_beauty/src')\n",
        "if src_path not in sys.path: sys.path.insert(0, src_path)\n",
        "\n",
        "from tiger_semantic_id_amazon_beauty.src.utils import set_seed, ensure_dirs, Paths\n",
        "set_seed(42)\n",
        "ensure_dirs(Paths.data_dir, Paths.artifacts_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Config dataclass\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    dataset_name: str = 'Beauty'\n",
        "    min_user_interactions: int = 5\n",
        "    max_hist_len: int = 20\n",
        "    embed_model_name: str = 'sentence-t5-base'\n",
        "    rqvae_latent_dim: int = 32\n",
        "    rqvae_levels: int = 3\n",
        "    rqvae_codebook_size: int = 256\n",
        "    rqvae_beta: float = 0.25\n",
        "    rqvae_epochs: int = 50\n",
        "    rqvae_batch_size: int = 1024\n",
        "    rqvae_lr: float = 4e-1\n",
        "    seq2seq_d_model: int = 128\n",
        "    seq2seq_ff: int = 1024\n",
        "    seq2seq_heads: int = 6\n",
        "    seq2seq_layers_enc: int = 4\n",
        "    seq2seq_layers_dec: int = 4\n",
        "    seq2seq_dropout: float = 0.1\n",
        "    seq2seq_batch_size: int = 256\n",
        "    seq2seq_steps: int = 20000\n",
        "    seq2seq_lr: float = 1e-2\n",
        "    user_vocab_hash: int = 2000\n",
        "    topk_list: tuple = (5, 10)\n",
        "\n",
        "cfg = Config()\n",
        "cfg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data\n",
        "from tiger_semantic_id_amazon_beauty.src.data import SNAP_REVIEWS, SNAP_META\n",
        "from tiger_semantic_id_amazon_beauty.src.utils import Paths\n",
        "!cd /content 2>/dev/null || true\n",
        "!mkdir -p {Paths.data_dir}\n",
        "!wget -q -O {Paths.data_dir}/reviews_Beauty_5.json.gz {SNAP_REVIEWS}\n",
        "!wget -q -O {Paths.data_dir}/meta_Beauty.json.gz {SNAP_META}\n",
        "!gzip -t {Paths.data_dir}/reviews_Beauty_5.json.gz && gzip -t {Paths.data_dir}/meta_Beauty.json.gz && echo 'gz ok'\n",
        "!zcat -f {Paths.data_dir}/reviews_Beauty_5.json.gz | head -n 2\n",
        "!zcat -f {Paths.data_dir}/meta_Beauty.json.gz | head -n 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse and preprocess\n",
        "import pandas as pd\n",
        "from tiger_semantic_id_amazon_beauty.src.data import load_reviews_df, load_meta_df, filter_and_split, build_id_maps, apply_id_maps, save_mappings\n",
        "reviews = load_reviews_df(f\"{Paths.data_dir}/reviews_Beauty_5.json.gz\")\n",
        "meta = load_meta_df(f\"{Paths.data_dir}/meta_Beauty.json.gz\")\n",
        "# Merge item_idx later after mapping\n",
        "train_df, val_df, test_df = filter_and_split(reviews, __import__('tiger_semantic_id_amazon_beauty.src.data', fromlist=['BeautyConfig']).BeautyConfig())\n",
        "user2id, item2id = build_id_maps([train_df, val_df, test_df])\n",
        "save_mappings(Paths.artifacts_dir, user2id, item2id)\n",
        "train_df = apply_id_maps(train_df, user2id, item2id)\n",
        "val_df = apply_id_maps(val_df, user2id, item2id)\n",
        "test_df = apply_id_maps(test_df, user2id, item2id)\n",
        "items = meta.merge(pd.DataFrame({'item_id': list(item2id.keys()), 'item_idx': list(item2id.values())}), on='item_id', how='right')\n",
        "print('Shapes:', train_df.shape, val_df.shape, test_df.shape, items.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build item text & embed with Sentence-T5\n",
        "import torch\n",
        "from tiger_semantic_id_amazon_beauty.src.embeddings import build_item_text, encode_items\n",
        "texts = build_item_text(items)\n",
        "item_emb = encode_items(texts, model_name=cfg.embed_model_name, batch_size=256)\n",
        "torch.save(item_emb, f\"{Paths.artifacts_dir}/item_embeddings.pt\")\n",
        "item_emb.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RQ-VAE training\n",
        "import torch\n",
        "from tiger_semantic_id_amazon_beauty.src.rqvae import RQVAE, RQVAEConfig, train_rqvae, encode_codes\n",
        "rqcfg = RQVAEConfig(input_dim=item_emb.shape[1], latent_dim=cfg.rqvae_latent_dim, levels=cfg.rqvae_levels, codebook_size=cfg.rqvae_codebook_size)\n",
        "rqcfg.beta = cfg.rqvae_beta\n",
        "model = RQVAE(rqcfg)\n",
        "model = train_rqvae(model, item_emb, epochs=cfg.rqvae_epochs, batch_size=cfg.rqvae_batch_size, lr=cfg.rqvae_lr)\n",
        "torch.save(model.state_dict(), f\"{Paths.artifacts_dir}/rqvae.pt\")\n",
        "codes = encode_codes(model, item_emb)\n",
        "codes.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assign Semantic IDs & save maps\n",
        "import numpy as np\n",
        "from tiger_semantic_id_amazon_beauty.src.semantic_id import assign_semantic_ids\n",
        "sid, sid_to_items, prefix_to_items = assign_semantic_ids(codes, Paths.artifacts_dir, codebook_size=cfg.rqvae_codebook_size)\n",
        "sid.shape, len(sid_to_items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations: c1 vs category, and hierarchy\n",
        "from tiger_semantic_id_amazon_beauty.src.visualize import plot_c1_category_distribution, plot_hierarchy_c1_c2\n",
        "fig1 = plot_c1_category_distribution(codes.numpy(), items)\n",
        "fig1.savefig(f\"{Paths.artifacts_dir}/figs_c1_category.png\")\n",
        "c1_vals = list(pd.Series(codes[:,0].numpy()).value_counts().head(3).index)\n",
        "fig2 = plot_hierarchy_c1_c2(codes.numpy(), items, c1_vals)\n",
        "fig2.savefig(f\"{Paths.artifacts_dir}/figs_hierarchy.png\")\n",
        "fig1, fig2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sequence dataset for generative retrieval\n",
        "from collections import defaultdict\n",
        "from tiger_semantic_id_amazon_beauty.src.seq2seq import TIGERSeqDataset, VocabConfig, Seq2SeqConfig\n",
        "user_hist = defaultdict(list)\n",
        "for r in train_df.sort_values(['user_idx','ts']).itertuples(index=False):\n",
        "    user_hist[int(r.user_idx)].append(int(r.item_idx))\n",
        "vocab_cfg = VocabConfig(codebook_size=cfg.rqvae_codebook_size, levels=4, user_vocab_hash=cfg.user_vocab_hash)\n",
        "seq_cfg = Seq2SeqConfig(d_model=cfg.seq2seq_d_model, ff=cfg.seq2seq_ff, heads=cfg.seq2seq_heads, layers_enc=cfg.seq2seq_layers_enc, layers_dec=cfg.seq2seq_layers_dec, dropout=cfg.seq2seq_dropout, max_hist_len=cfg.max_hist_len, batch_size=cfg.seq2seq_batch_size, lr=cfg.seq2seq_lr)\n",
        "train_ds = TIGERSeqDataset(user_hist, sid, user_hash_size=vocab_cfg.user_vocab_hash, codebook_size=vocab_cfg.codebook_size, max_hist_len=seq_cfg.max_hist_len)\n",
        "len(train_ds), train_ds[0][1][:8], train_ds[0][2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seq2Seq model & training (compact)\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tiger_semantic_id_amazon_beauty.src.seq2seq import TinyTransformer, collate_batch\n",
        "V = 1 + vocab_cfg.semantic_vocab + vocab_cfg.user_vocab_hash + 2  # PAD=0, BOS=1, then others\n",
        "model = TinyTransformer(vocab_size=V, d_model=seq_cfg.d_model, ff=seq_cfg.ff, heads=seq_cfg.heads, layers_enc=seq_cfg.layers_enc, layers_dec=seq_cfg.layers_dec, dropout=seq_cfg.dropout)\n",
        "model = model.cuda() if torch.cuda.is_available() else model\n",
        "opt = torch.optim.Adam(model.parameters(), lr=seq_cfg.lr)\n",
        "loader = DataLoader(train_ds, batch_size=seq_cfg.batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "steps = 0\n",
        "for src, tgt in loader:\n",
        "    if torch.cuda.is_available(): src, tgt = src.cuda(), tgt.cuda()\n",
        "    logits = model(src, tgt[:, :-1])\n",
        "    loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1), ignore_index=0)\n",
        "    opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
        "    steps += 1\n",
        "    if steps % 200 == 0:\n",
        "        print('steps', steps, 'loss', float(loss))\n",
        "    if steps >= 1000: break  # knob for Colab runtime\n",
        "torch.save(model.state_dict(), f\"{Paths.artifacts_dir}/seq2seq.pt\")\n",
        "steps\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
