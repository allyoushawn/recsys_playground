{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIGER SemanticID on Amazon Beauty — Experiment Plan\n",
    "\n",
    "Goal: Implement Semantic IDs via RQ-VAE and a compact seq2seq Transformer for generative retrieval on Amazon Beauty 5-core; produce metrics and visualizations validating paper claims.\n",
    "\n",
    "Datasets: Amazon Product Reviews (Beauty, 5-core).\n",
    "\n",
    "Key steps: Download & preprocess; Sentence-T5 embeddings; RQ-VAE (3 levels, K=256) to 3-tuple codes + collision code c4; visualizations (c1↔category, hierarchy); seq2seq generative retrieval; metrics Recall@5/10, NDCG@5/10 and invalid-ID rate; ablations (Random/LSH); mini cold-start probe.\n",
    "\n",
    "Artifacts: save to /content/artifacts. Keep configs modest for Colab; add knobs for smoke tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo, install dependencies, and make src importable (Colab-friendly)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "repo_url = 'https://github.com/allyoushawn/recsys_playground.git'\n",
    "repo_dir = 'recsys_playground'\n",
    "branch_name = '20250908_tiger_dev'\n",
    "\n",
    "import os\n",
    "if IN_COLAB:\n",
    "    if os.path.exists(repo_dir):\n",
    "        ! rm -rf $repo_dir\n",
    "    !git clone $repo_url\n",
    "    %cd $repo_dir\n",
    "    !git fetch --all\n",
    "    !git checkout $branch_name || echo 'Branch not found; staying on default.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime & installs\n",
    "import os, sys, subprocess, torch\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# Install module dependencies (Colab).\n",
    "!pip -q install -r tiger_semantic_id_amazon_beauty/requirements.txt\n",
    "\n",
    "# Make src importable\n",
    "src_path = os.path.abspath('tiger_semantic_id_amazon_beauty/src')\n",
    "if src_path not in sys.path: sys.path.insert(0, src_path)\n",
    "\n",
    "from tiger_semantic_id_amazon_beauty.src.utils import set_seed, ensure_dirs, Paths\n",
    "set_seed(42)\n",
    "ensure_dirs(Paths.data_dir, Paths.artifacts_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11166c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config dataclass\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    dataset_name: str = 'Beauty'\n",
    "    min_user_interactions: int = 5\n",
    "    max_hist_len: int = 20\n",
    "    embed_model_name: str = 'sentence-t5-base'\n",
    "    rqvae_latent_dim: int = 32\n",
    "    rqvae_levels: int = 3\n",
    "    rqvae_codebook_size: int = 256\n",
    "    rqvae_beta: float = 0.25\n",
    "    rqvae_epochs: int = 20  # Reduced from 50 for faster testing\n",
    "    rqvae_batch_size: int = 1024\n",
    "    rqvae_lr: float = 1e-3  # FIXED: Reduced from 4e-1 to 1e-3 for stability\n",
    "    seq2seq_d_model: int = 128\n",
    "    seq2seq_ff: int = 1024\n",
    "    seq2seq_heads: int = 8  # Changed from 6 to 8 so it divides 128 evenly\n",
    "    seq2seq_layers_enc: int = 4\n",
    "    seq2seq_layers_dec: int = 4\n",
    "    seq2seq_dropout: float = 0.1\n",
    "    seq2seq_batch_size: int = 256\n",
    "    seq2seq_steps: int = 20000\n",
    "    seq2seq_lr: float = 1e-2\n",
    "    user_vocab_hash: int = 2000\n",
    "    topk_list: tuple = (5, 10)\n",
    "\n",
    "cfg = Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for Python dict format in metadata files BEFORE any data imports\n",
    "def _parse_python_dict_lines(path: str):\n",
    "    \"\"\"Parse Python dict lines (not JSON) from a gzipped file using ast.literal_eval.\"\"\"\n",
    "    import ast\n",
    "    import gzip\n",
    "    \n",
    "    opener = gzip.open if path.endswith(\".gz\") else open\n",
    "    rows = []\n",
    "    with opener(path, \"rt\") as f:\n",
    "        for raw in f:\n",
    "            try:\n",
    "                line = raw.strip()\n",
    "                if line:\n",
    "                    # Use ast.literal_eval to safely parse Python dict strings\n",
    "                    data = ast.literal_eval(line)\n",
    "                    rows.append(data)\n",
    "            except (ValueError, SyntaxError, MemoryError):\n",
    "                # Skip malformed lines\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "# Apply the fix BEFORE importing data functions\n",
    "from tiger_semantic_id_amazon_beauty.src import data\n",
    "data._parse_json_lines = _parse_python_dict_lines\n",
    "print(\"✓ Applied Python dict parser fix\")\n",
    "\n",
    "# Now import data functions and download data\n",
    "from tiger_semantic_id_amazon_beauty.src.data import SNAP_REVIEWS, SNAP_META\n",
    "from tiger_semantic_id_amazon_beauty.src.utils import Paths\n",
    "!cd /content 2>/dev/null || true\n",
    "!mkdir -p {Paths.data_dir}\n",
    "!wget -q -O {Paths.data_dir}/reviews_Beauty_5.json.gz {SNAP_REVIEWS}\n",
    "!wget -q -O {Paths.data_dir}/meta_Beauty.json.gz {SNAP_META}\n",
    "!gzip -t {Paths.data_dir}/reviews_Beauty_5.json.gz && gzip -t {Paths.data_dir}/meta_Beauty.json.gz && echo 'gz ok'\n",
    "!zcat -f {Paths.data_dir}/reviews_Beauty_5.json.gz | head -n 2\n",
    "!zcat -f {Paths.data_dir}/meta_Beauty.json.gz | head -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and preprocess\n",
    "import pandas as pd\n",
    "from tiger_semantic_id_amazon_beauty.src.data import load_reviews_df, load_meta_df, filter_and_split, build_id_maps, apply_id_maps, save_mappings\n",
    "reviews = load_reviews_df(f\"{Paths.data_dir}/reviews_Beauty_5.json.gz\")\n",
    "meta = load_meta_df(f\"{Paths.data_dir}/meta_Beauty.json.gz\")\n",
    "# Merge item_idx later after mapping\n",
    "train_df, val_df, test_df = filter_and_split(reviews, __import__('tiger_semantic_id_amazon_beauty.src.data', fromlist=['BeautyConfig']).BeautyConfig())\n",
    "user2id, item2id = build_id_maps([train_df, val_df, test_df])\n",
    "save_mappings(Paths.artifacts_dir, user2id, item2id)\n",
    "train_df = apply_id_maps(train_df, user2id, item2id)\n",
    "val_df = apply_id_maps(val_df, user2id, item2id)\n",
    "test_df = apply_id_maps(test_df, user2id, item2id)\n",
    "# Robust merge: ensure metadata has 'item_id' even if source used 'asin'\n",
    "meta_merge = meta.copy()\n",
    "print(\"Meta columns:\", meta.columns.tolist())\n",
    "print(\"Meta shape:\", meta.shape)\n",
    "items = pd.DataFrame({'item_id': list(item2id.keys()), 'item_idx': list(item2id.values())}).merge(meta, on='item_id', how='left')\n",
    "print('Shapes:', train_df.shape, val_df.shape, test_df.shape, items.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build item text & embed with Sentence-T5 (GPU-optimized)\n",
    "import torch\n",
    "from tiger_semantic_id_amazon_beauty.src.embeddings import build_item_text, encode_items\n",
    "\n",
    "# Build item texts from metadata\n",
    "texts = build_item_text(items)\n",
    "print(f\"Built {len(texts)} item text descriptions\")\n",
    "\n",
    "# Encode with GPU acceleration (auto-detects CUDA if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Encoding items with device: {device}\")\n",
    "\n",
    "item_emb = encode_items(\n",
    "    texts, \n",
    "    model_name=cfg.embed_model_name, \n",
    "    batch_size=256 if device == \"cuda\" else 128,  # Larger batch size for GPU\n",
    "    device=device  # Explicit device specification\n",
    ")\n",
    "\n",
    "# Save embeddings to disk\n",
    "torch.save(item_emb, f\"{Paths.artifacts_dir}/item_embeddings.pt\")\n",
    "print(f\"Saved embeddings to {Paths.artifacts_dir}/item_embeddings.pt\")\n",
    "print(f\"Embeddings device: {item_emb.device}\")\n",
    "\n",
    "# Debug: Check if the input embeddings themselves are diverse\n",
    "print(\"\\n=== INPUT DATA ANALYSIS ===\")\n",
    "sample_items = item_emb[:10]\n",
    "print(f\"First 10 item embeddings are identical? {torch.allclose(sample_items[0], sample_items[1])}\")\n",
    "print(f\"All 10 embeddings identical? {all(torch.allclose(sample_items[0], sample_items[i]) for i in range(1, 10))}\")\n",
    "\n",
    "# Check actual values\n",
    "print(f\"Item 0 first 10 dims: {sample_items[0][:10]}\")\n",
    "print(f\"Item 1 first 10 dims: {sample_items[1][:10]}\")\n",
    "print(f\"Item 2 first 10 dims: {sample_items[2][:10]}\")\n",
    "\n",
    "# Check if there's variance within each embedding\n",
    "for i in range(5):\n",
    "    print(f\"Item {i} internal variance: {sample_items[i].var():.6f}\")\n",
    "\n",
    "print(f\"\\nFinal embedding shape: {item_emb.shape}\")\n",
    "print(f\"Embedding device: {item_emb.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "odv8k0edl3g",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The improved RQ-VAE architecture is now integrated into the main RQVAE class in rqvae.py\n",
    "# No need for separate ImprovedRQVAE class or train_rqvae patches\n",
    "print(\"✓ Using improved RQ-VAE architecture from rqvae.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RQ-VAE training with improved architecture (GPU-optimized)\n",
    "import torch\n",
    "from tiger_semantic_id_amazon_beauty.src.rqvae import RQVAE, RQVAEConfig, train_rqvae, encode_codes\n",
    "\n",
    "# Setup device for GPU acceleration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"RQ-VAE training will use device: {device}\")\n",
    "\n",
    "# Create RQ-VAE configuration\n",
    "rqcfg = RQVAEConfig(input_dim=item_emb.shape[1], latent_dim=cfg.rqvae_latent_dim, levels=cfg.rqvae_levels, codebook_size=cfg.rqvae_codebook_size)\n",
    "rqcfg.beta = cfg.rqvae_beta\n",
    "\n",
    "print(\"=== CREATING IMPROVED RQ-VAE MODEL ===\")\n",
    "model = RQVAE(rqcfg).to(device)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "# Move embeddings to same device as training will occur\n",
    "if item_emb.device != torch.device(device):\n",
    "    print(f\"Moving embeddings from {item_emb.device} to {device}\")\n",
    "    item_emb = item_emb.to(device)\n",
    "else:\n",
    "    print(f\"Embeddings already on {device}\")\n",
    "\n",
    "print(\"\\n=== TESTING MODEL DIVERSITY (PRE-TRAINING) ===\") \n",
    "# Test diversity before training to ensure the improved architecture works\n",
    "with torch.no_grad():\n",
    "    # Test encoder diversity\n",
    "    encoded = model.encoder(model.normalize(item_emb[:20]))\n",
    "    dists = torch.cdist(encoded[:10], encoded[:10])\n",
    "    print(f\"Encoder diversity: mean pairwise distance = {dists.fill_diagonal_(float('inf')).mean():.6f}\")\n",
    "    \n",
    "    # Test quantization diversity \n",
    "    z = model.encoder(model.normalize(item_emb[:100]))\n",
    "    q, codes = model.codebook(z)\n",
    "    unique_codes = len(torch.unique(codes, dim=0))\n",
    "    print(f\"Quantization diversity: {unique_codes} unique codes out of 100 items\")\n",
    "    print(f\"Sample codes: {codes[:5].tolist()}\")\n",
    "\n",
    "# Run RQ-VAE training with GPU acceleration\n",
    "print(f\"\\n=== STARTING RQ-VAE TRAINING ON {device.upper()} ===\")\n",
    "print(f\"Training parameters: epochs={cfg.rqvae_epochs}, batch_size={cfg.rqvae_batch_size}, lr={cfg.rqvae_lr}\")\n",
    "\n",
    "# Adjust batch size based on device (larger for GPU)\n",
    "training_batch_size = cfg.rqvae_batch_size if device == \"cuda\" else min(cfg.rqvae_batch_size, 512)\n",
    "if training_batch_size != cfg.rqvae_batch_size:\n",
    "    print(f\"Adjusted batch size for {device}: {cfg.rqvae_batch_size} -> {training_batch_size}\")\n",
    "\n",
    "model = train_rqvae(\n",
    "    model, \n",
    "    item_emb, \n",
    "    epochs=cfg.rqvae_epochs, \n",
    "    batch_size=training_batch_size, \n",
    "    lr=cfg.rqvae_lr,\n",
    "    device=device  # Explicit device specification\n",
    ")\n",
    "\n",
    "# Save trained model\n",
    "torch.save(model.state_dict(), f\"{Paths.artifacts_dir}/rqvae.pt\")\n",
    "print(f\"Saved trained model to {Paths.artifacts_dir}/rqvae.pt\")\n",
    "\n",
    "# Generate codes for the full dataset  \n",
    "print(\"\\n=== GENERATING SEMANTIC CODES ===\")\n",
    "codes = encode_codes(model, item_emb, device=device)\n",
    "final_unique = len(torch.unique(codes, dim=0))\n",
    "print(f\"Final codes shape: {codes.shape}\")\n",
    "print(f\"Final unique codes: {final_unique} out of {len(codes)} items ({100*final_unique/len(codes):.1f}% diversity)\")\n",
    "print(f\"Sample final codes: {codes[:5].tolist()}\")\n",
    "\n",
    "# Check if diversity was preserved through training\n",
    "if final_unique > len(codes) * 0.8:  # More than 80% unique\n",
    "    print(\"✅ Excellent code diversity preserved through training!\")\n",
    "elif final_unique > len(codes) * 0.5:  # More than 50% unique  \n",
    "    print(\"✅ Good code diversity maintained\")\n",
    "else:\n",
    "    print(\"⚠️  Code diversity may need improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Semantic IDs & save maps\n",
    "import numpy as np\n",
    "from tiger_semantic_id_amazon_beauty.src.semantic_id import assign_semantic_ids\n",
    "sid, sid_to_items, prefix_to_items = assign_semantic_ids(codes, Paths.artifacts_dir, codebook_size=cfg.rqvae_codebook_size)\n",
    "sid.shape, len(sid_to_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations: c1 vs category, and hierarchy\n",
    "from tiger_semantic_id_amazon_beauty.src.visualize import plot_c1_category_distribution, plot_hierarchy_c1_c2\n",
    "fig1 = plot_c1_category_distribution(codes.numpy(), items)\n",
    "fig1.savefig(f\"{Paths.artifacts_dir}/figs_c1_category.png\")\n",
    "c1_vals = list(pd.Series(codes[:,0].numpy()).value_counts().head(3).index)\n",
    "fig2 = plot_hierarchy_c1_c2(codes.numpy(), items, c1_vals)\n",
    "fig2.savefig(f\"{Paths.artifacts_dir}/figs_hierarchy.png\")\n",
    "fig1, fig2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence dataset for generative retrieval\n",
    "from collections import defaultdict\n",
    "from tiger_semantic_id_amazon_beauty.src.seq2seq import TIGERSeqDataset, VocabConfig, Seq2SeqConfig\n",
    "user_hist = defaultdict(list)\n",
    "for r in train_df.sort_values(['user_idx','ts']).itertuples(index=False):\n",
    "    user_hist[int(r.user_idx)].append(int(r.item_idx))\n",
    "# Fix: use cfg.rqvae_levels to match the RQ-VAE configuration (3 levels)\n",
    "vocab_cfg = VocabConfig(codebook_size=cfg.rqvae_codebook_size, levels=cfg.rqvae_levels, user_vocab_hash=cfg.user_vocab_hash)\n",
    "seq_cfg = Seq2SeqConfig(d_model=cfg.seq2seq_d_model, ff=cfg.seq2seq_ff, heads=cfg.seq2seq_heads, layers_enc=cfg.seq2seq_layers_enc, layers_dec=cfg.seq2seq_layers_dec, dropout=cfg.seq2seq_dropout, max_hist_len=cfg.max_hist_len, batch_size=cfg.seq2seq_batch_size, lr=cfg.seq2seq_lr)\n",
    "train_ds = TIGERSeqDataset(user_hist, sid, user_hash_size=vocab_cfg.user_vocab_hash, codebook_size=vocab_cfg.codebook_size, max_hist_len=seq_cfg.max_hist_len)\n",
    "len(train_ds), train_ds[0][1][:8], train_ds[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq model & training (compact)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tiger_semantic_id_amazon_beauty.src.seq2seq import TinyTransformer, collate_batch\n",
    "V = 1 + vocab_cfg.semantic_vocab + vocab_cfg.user_vocab_hash + 2  # PAD=0, BOS=1, then others\n",
    "model = TinyTransformer(vocab_size=V, d_model=seq_cfg.d_model, ff=seq_cfg.ff, heads=seq_cfg.heads, layers_enc=seq_cfg.layers_enc, layers_dec=seq_cfg.layers_dec, dropout=seq_cfg.dropout)\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n",
    "opt = torch.optim.Adam(model.parameters(), lr=seq_cfg.lr)\n",
    "loader = DataLoader(train_ds, batch_size=seq_cfg.batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "steps = 0\n",
    "for src, tgt in loader:\n",
    "    if torch.cuda.is_available(): src, tgt = src.cuda(), tgt.cuda()\n",
    "    logits = model(src, tgt[:, :-1])\n",
    "    loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1), ignore_index=0)\n",
    "    opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
    "    steps += 1\n",
    "    if steps % 200 == 0:\n",
    "        print('steps', steps, 'loss', float(loss))\n",
    "    if steps >= 1000: break  # knob for Colab runtime\n",
    "torch.save(model.state_dict(), f\"{Paths.artifacts_dir}/seq2seq.pt\")\n",
    "steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
