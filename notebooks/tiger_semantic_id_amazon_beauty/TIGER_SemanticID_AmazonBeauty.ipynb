{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIGER SemanticID on Amazon Beauty — Experiment Plan\n",
    "\n",
    "Goal: Implement Semantic IDs via RQ-VAE and a compact seq2seq Transformer for generative retrieval on Amazon Beauty 5-core; produce metrics and visualizations validating paper claims.\n",
    "\n",
    "Datasets: Amazon Product Reviews (Beauty, 5-core).\n",
    "\n",
    "Key steps: Download & preprocess; Sentence-T5 embeddings; RQ-VAE (3 levels, K=256) to 3-tuple codes + collision code c4; visualizations (c1↔category, hierarchy); seq2seq generative retrieval; metrics Recall@5/10, NDCG@5/10 and invalid-ID rate; ablations (Random/LSH); mini cold-start probe.\n",
    "\n",
    "Artifacts: save to /content/artifacts. Keep configs modest for Colab; add knobs for smoke tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo, install dependencies, and make src importable (Colab-friendly)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "repo_url = 'https://github.com/allyoushawn/recsys_playground.git'\n",
    "repo_dir = 'recsys_playground'\n",
    "branch_name = '20250908_tiger_dev'\n",
    "\n",
    "import os\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists(repo_dir):\n",
    "        !git clone $repo_url\n",
    "    %cd $repo_dir\n",
    "    !git fetch --all\n",
    "    !git checkout $branch_name || echo 'Branch not found; staying on default.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime & installs\n",
    "import os, sys, subprocess, torch\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# Install module dependencies (Colab).\n",
    "!pip -q install -r tiger_semantic_id_amazon_beauty/requirements.txt\n",
    "\n",
    "# Make src importable\n",
    "src_path = os.path.abspath('tiger_semantic_id_amazon_beauty/src')\n",
    "if src_path not in sys.path: sys.path.insert(0, src_path)\n",
    "\n",
    "from tiger_semantic_id_amazon_beauty.src.utils import set_seed, ensure_dirs, Paths\n",
    "set_seed(42)\n",
    "ensure_dirs(Paths.data_dir, Paths.artifacts_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11166c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config dataclass\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    dataset_name: str = 'Beauty'\n",
    "    min_user_interactions: int = 5\n",
    "    max_hist_len: int = 20\n",
    "    embed_model_name: str = 'sentence-t5-base'\n",
    "    rqvae_latent_dim: int = 32\n",
    "    rqvae_levels: int = 3\n",
    "    rqvae_codebook_size: int = 256\n",
    "    rqvae_beta: float = 0.25\n",
    "    rqvae_epochs: int = 20  # Reduced from 50 for faster testing\n",
    "    rqvae_batch_size: int = 1024\n",
    "    rqvae_lr: float = 1e-3  # FIXED: Reduced from 4e-1 to 1e-3 for stability\n",
    "    seq2seq_d_model: int = 128\n",
    "    seq2seq_ff: int = 1024\n",
    "    seq2seq_heads: int = 8  # Changed from 6 to 8 so it divides 128 evenly\n",
    "    seq2seq_layers_enc: int = 4\n",
    "    seq2seq_layers_dec: int = 4\n",
    "    seq2seq_dropout: float = 0.1\n",
    "    seq2seq_batch_size: int = 256\n",
    "    seq2seq_steps: int = 20000\n",
    "    seq2seq_lr: float = 1e-2\n",
    "    user_vocab_hash: int = 2000\n",
    "    topk_list: tuple = (5, 10)\n",
    "\n",
    "cfg = Config()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3374d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for Python dict format in metadata files BEFORE any data imports\n",
    "def _parse_python_dict_lines(path: str):\n",
    "    \"\"\"Parse Python dict lines (not JSON) from a gzipped file using ast.literal_eval.\"\"\"\n",
    "    import ast\n",
    "    import gzip\n",
    "    \n",
    "    opener = gzip.open if path.endswith(\".gz\") else open\n",
    "    rows = []\n",
    "    with opener(path, \"rt\") as f:\n",
    "        for raw in f:\n",
    "            try:\n",
    "                line = raw.strip()\n",
    "                if line:\n",
    "                    # Use ast.literal_eval to safely parse Python dict strings\n",
    "                    data = ast.literal_eval(line)\n",
    "                    rows.append(data)\n",
    "            except (ValueError, SyntaxError, MemoryError):\n",
    "                # Skip malformed lines\n",
    "                continue\n",
    "    return rows\n",
    "\n",
    "# Apply the fix BEFORE importing data functions\n",
    "from tiger_semantic_id_amazon_beauty.src import data\n",
    "data._parse_json_lines = _parse_python_dict_lines\n",
    "print(\"✓ Applied Python dict parser fix\")\n",
    "\n",
    "# Now import data functions and download data\n",
    "from tiger_semantic_id_amazon_beauty.src.data import SNAP_REVIEWS, SNAP_META\n",
    "from tiger_semantic_id_amazon_beauty.src.utils import Paths\n",
    "!cd /content 2>/dev/null || true\n",
    "!mkdir -p {Paths.data_dir}\n",
    "!wget -q -O {Paths.data_dir}/reviews_Beauty_5.json.gz {SNAP_REVIEWS}\n",
    "!wget -q -O {Paths.data_dir}/meta_Beauty.json.gz {SNAP_META}\n",
    "!gzip -t {Paths.data_dir}/reviews_Beauty_5.json.gz && gzip -t {Paths.data_dir}/meta_Beauty.json.gz && echo 'gz ok'\n",
    "!zcat -f {Paths.data_dir}/reviews_Beauty_5.json.gz | head -n 2\n",
    "!zcat -f {Paths.data_dir}/meta_Beauty.json.gz | head -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and preprocess\n",
    "import pandas as pd\n",
    "from tiger_semantic_id_amazon_beauty.src.data import load_reviews_df, load_meta_df, filter_and_split, build_id_maps, apply_id_maps, save_mappings\n",
    "reviews = load_reviews_df(f\"{Paths.data_dir}/reviews_Beauty_5.json.gz\")\n",
    "meta = load_meta_df(f\"{Paths.data_dir}/meta_Beauty.json.gz\")\n",
    "# Merge item_idx later after mapping\n",
    "train_df, val_df, test_df = filter_and_split(reviews, __import__('tiger_semantic_id_amazon_beauty.src.data', fromlist=['BeautyConfig']).BeautyConfig())\n",
    "user2id, item2id = build_id_maps([train_df, val_df, test_df])\n",
    "save_mappings(Paths.artifacts_dir, user2id, item2id)\n",
    "train_df = apply_id_maps(train_df, user2id, item2id)\n",
    "val_df = apply_id_maps(val_df, user2id, item2id)\n",
    "test_df = apply_id_maps(test_df, user2id, item2id)\n",
    "# Robust merge: ensure metadata has 'item_id' even if source used 'asin'\n",
    "meta_merge = meta.copy()\n",
    "print(\"Meta columns:\", meta.columns.tolist())\n",
    "print(\"Meta shape:\", meta.shape)\n",
    "items = pd.DataFrame({'item_id': list(item2id.keys()), 'item_idx': list(item2id.values())}).merge(meta, on='item_id', how='left')\n",
    "print('Shapes:', train_df.shape, val_df.shape, test_df.shape, items.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25d0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build item text & embed with Sentence-T5\n",
    "import torch\n",
    "from tiger_semantic_id_amazon_beauty.src.embeddings import build_item_text, encode_items\n",
    "texts = build_item_text(items)\n",
    "item_emb = encode_items(texts, model_name=cfg.embed_model_name, batch_size=256)\n",
    "torch.save(item_emb, f\"{Paths.artifacts_dir}/item_embeddings.pt\")\n",
    "\n",
    "# Debug: Check if the input embeddings themselves are diverse\n",
    "print(\"=== INPUT DATA ANALYSIS ===\")\n",
    "sample_items = item_emb[:10]\n",
    "print(f\"First 10 item embeddings are identical? {torch.allclose(sample_items[0], sample_items[1])}\")\n",
    "print(f\"All 10 embeddings identical? {all(torch.allclose(sample_items[0], sample_items[i]) for i in range(1, 10))}\")\n",
    "\n",
    "# Check actual values\n",
    "print(f\"Item 0 first 10 dims: {sample_items[0][:10]}\")\n",
    "print(f\"Item 1 first 10 dims: {sample_items[1][:10]}\")\n",
    "print(f\"Item 2 first 10 dims: {sample_items[2][:10]}\")\n",
    "\n",
    "# Check if there's variance within each embedding\n",
    "for i in range(5):\n",
    "    print(f\"Item {i} internal variance: {sample_items[i].var():.6f}\")\n",
    "\n",
    "item_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "odv8k0edl3g",
   "metadata": {},
   "outputs": [],
   "source": "# Fix for RQ-VAE tensor dimension mismatch AND training stability + Encoder Initialization\ndef improved_init_weights(m):\n    \"\"\"Improved weight initialization to preserve diversity through encoder layers\"\"\"\n    if isinstance(m, torch.nn.Linear):\n        # Use He initialization with larger scale for better diversity preservation\n        torch.nn.init.kaiming_uniform_(m.weight, mode='fan_out', nonlinearity='relu')\n        if m.bias is not None:\n            torch.nn.init.constant_(m.bias, 0)\n\ndef fixed_train_rqvae(\n    model, data, epochs=50, batch_size=1024, lr=1e-3,\n    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n):\n    model = model.to(device)\n    data = data.to(device)\n    \n    # Fix 1: Normalize input data to prevent collapse\n    data_mean = data.mean(dim=0, keepdim=True)\n    data_std = data.std(dim=0, keepdim=True) + 1e-8  # Add small epsilon to prevent division by zero\n    data = (data - data_mean) / data_std\n    \n    # Fix 2: Improved weight initialization for better diversity\n    model.apply(improved_init_weights)\n    \n    opt = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    # K-means init on a sample batch - FIXED VERSION\n    with torch.no_grad():\n        sample = data[torch.randperm(data.shape[0])[: min(batch_size, data.shape[0])]].to(device)\n        # Encode the sample to get the correct latent dimension for kmeans init\n        encoded_sample = model.encoder(sample)\n        model.codebook.kmeans_init(encoded_sample)\n    \n    N = data.shape[0]\n    for ep in range(1, epochs + 1):\n        perm = torch.randperm(N, device=device)\n        total = 0.0\n        for i in range(0, N, batch_size):\n            idx = perm[i : i + batch_size]\n            xb = data[idx]\n            x_hat, loss, recon, _ = model(xb)\n            opt.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n            opt.step()\n            total += loss.item() * xb.size(0)\n        avg_loss = total / N\n        if ep % 5 == 0 or ep == 1:\n            print(f\"[RQVAE] epoch {ep}/{epochs} loss={avg_loss:.4f}\")\n        # Early stopping if loss becomes too small (indicating collapse)\n        if avg_loss < 1e-6:\n            print(f\"[RQVAE] Early stopping at epoch {ep} due to loss collapse\")\n            break\n    return model\n\n# Alternative: Create a better encoder architecture\nclass ImprovedRQVAE(torch.nn.Module):\n    \"\"\"RQ-VAE with improved encoder that preserves diversity better\"\"\"\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        # Shallower encoder with skip connections to preserve diversity\n        self.encoder = torch.nn.Sequential(\n            torch.nn.Linear(cfg.input_dim, 256),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.1),\n            torch.nn.Linear(256, 128),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.1),\n            torch.nn.Linear(128, cfg.latent_dim)\n        )\n        self.decoder = torch.nn.Sequential(\n            torch.nn.Linear(cfg.latent_dim, 128),\n            torch.nn.ReLU(),\n            torch.nn.Linear(128, 256), \n            torch.nn.ReLU(),\n            torch.nn.Linear(256, cfg.input_dim)\n        )\n        from tiger_semantic_id_amazon_beauty.src.rqvae import RQCodebook\n        self.codebook = RQCodebook(cfg.levels, cfg.codebook_size, cfg.latent_dim)\n        \n        # Better initialization\n        self.apply(improved_init_weights)\n    \n    def forward(self, x):\n        z = self.encoder(x)\n        q, codes = self.codebook(z)\n        x_hat = self.decoder(q)\n        # Losses\n        recon = torch.nn.functional.mse_loss(x_hat, x)\n        # VQ losses: commit + codebook (stop-grad on one side)\n        commit = torch.nn.functional.mse_loss(z.detach(), q)\n        code = torch.nn.functional.mse_loss(z, q.detach())\n        loss = recon + self.cfg.beta * (commit + code)\n        return x_hat, loss, recon, codes\n\n# Replace the train_rqvae function\nfrom tiger_semantic_id_amazon_beauty.src import rqvae\nrqvae.train_rqvae = fixed_train_rqvae\n\nprint(\"✓ Applied improved RQ-VAE initialization and architecture fixes\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5722b",
   "metadata": {},
   "outputs": [],
   "source": "# RQ-VAE training with improved architecture\nimport torch\nfrom tiger_semantic_id_amazon_beauty.src.rqvae import RQVAE, RQVAEConfig, train_rqvae, encode_codes\nrqcfg = RQVAEConfig(input_dim=item_emb.shape[1], latent_dim=cfg.rqvae_latent_dim, levels=cfg.rqvae_levels, codebook_size=cfg.rqvae_codebook_size)\nrqcfg.beta = cfg.rqvae_beta\n\n# TEST: Compare original vs improved model\nprint(\"=== COMPARING ORIGINAL VS IMPROVED MODEL ===\")\n\n# Original model\nprint(\"\\\\n--- ORIGINAL MODEL ---\")\nmodel_orig = RQVAE(rqcfg)\nwith torch.no_grad():\n    encoded_orig = model_orig.encoder(item_emb[:20])\n    print(f\"Original encoder output diversity:\")\n    dists_orig = torch.cdist(encoded_orig[:10], encoded_orig[:10])\n    print(f\"  Mean pairwise distance: {dists_orig.fill_diagonal_(float('inf')).mean():.6f}\")\n    print(f\"  Min distance: {dists_orig.fill_diagonal_(float('inf')).min():.6f}\")\n    print(f\"  Max distance: {dists_orig.max():.6f}\")\n\n# Improved model\nprint(\"\\\\n--- IMPROVED MODEL ---\")\nmodel_improved = ImprovedRQVAE(rqcfg)\nwith torch.no_grad():\n    encoded_improved = model_improved.encoder(item_emb[:20])\n    print(f\"Improved encoder output diversity:\")\n    dists_improved = torch.cdist(encoded_improved[:10], encoded_improved[:10])\n    print(f\"  Mean pairwise distance: {dists_improved.fill_diagonal_(float('inf')).mean():.6f}\")\n    print(f\"  Min distance: {dists_improved.fill_diagonal_(float('inf')).min():.6f}\")\n    print(f\"  Max distance: {dists_improved.max():.6f}\")\n\n# Test quantization diversity\nprint(\"\\\\n--- QUANTIZATION DIVERSITY TEST ---\")\ndef test_quantization_diversity(model, data, name):\n    codes = []\n    with torch.no_grad():\n        encoded = model.encoder(data)\n        q, code_batch = model.codebook(encoded)\n        codes = code_batch\n    \n    unique_codes = torch.unique(codes, dim=0)\n    print(f\"{name} - Unique code combinations: {len(unique_codes)} out of {len(data)}\")\n    print(f\"  Sample codes: {codes[:5].tolist()}\")\n    return len(unique_codes)\n\norig_diversity = test_quantization_diversity(model_orig, item_emb[:100], \"Original\")\nimproved_diversity = test_quantization_diversity(model_improved, item_emb[:100], \"Improved\")\n\n# Use the better model\nif improved_diversity > orig_diversity:\n    print(f\"\\\\n✓ Using improved model (diversity: {improved_diversity} vs {orig_diversity})\")\n    model = model_improved\nelse:\n    print(f\"\\\\n✓ Using original model (diversity: {orig_diversity} vs {improved_diversity})\")\n    model = model_orig\n\nprint(\"\\\\n=== FINAL MODEL ANALYSIS ===\")\n# Final detailed analysis of chosen model\nwith torch.no_grad():\n    sample_encoded = model.encoder(item_emb[:10])\n    final_codes = encode_codes(model, item_emb[:50])\n    \nprint(f\"Final encoder diversity: mean pairwise distance = {torch.cdist(sample_encoded, sample_encoded).fill_diagonal_(float('inf')).mean():.6f}\")\nprint(f\"Final quantization: {len(torch.unique(final_codes, dim=0))} unique codes out of 50 items\")\nprint(f\"Sample final codes: {final_codes[:10]}\")\n\n# Skip training for now to test architecture\n# model = train_rqvae(model, item_emb, epochs=cfg.rqvae_epochs, batch_size=cfg.rqvae_batch_size, lr=cfg.rqvae_lr)\n# torch.save(model.state_dict(), f\"{Paths.artifacts_dir}/rqvae.pt\")\ncodes = final_codes[:50]  # Use for testing\nprint(f\"codes.shape {codes.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Semantic IDs & save maps\n",
    "import numpy as np\n",
    "from tiger_semantic_id_amazon_beauty.src.semantic_id import assign_semantic_ids\n",
    "sid, sid_to_items, prefix_to_items = assign_semantic_ids(codes, Paths.artifacts_dir, codebook_size=cfg.rqvae_codebook_size)\n",
    "sid.shape, len(sid_to_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations: c1 vs category, and hierarchy\n",
    "from tiger_semantic_id_amazon_beauty.src.visualize import plot_c1_category_distribution, plot_hierarchy_c1_c2\n",
    "fig1 = plot_c1_category_distribution(codes.numpy(), items)\n",
    "fig1.savefig(f\"{Paths.artifacts_dir}/figs_c1_category.png\")\n",
    "c1_vals = list(pd.Series(codes[:,0].numpy()).value_counts().head(3).index)\n",
    "fig2 = plot_hierarchy_c1_c2(codes.numpy(), items, c1_vals)\n",
    "fig2.savefig(f\"{Paths.artifacts_dir}/figs_hierarchy.png\")\n",
    "fig1, fig2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be88ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence dataset for generative retrieval\n",
    "from collections import defaultdict\n",
    "from tiger_semantic_id_amazon_beauty.src.seq2seq import TIGERSeqDataset, VocabConfig, Seq2SeqConfig\n",
    "user_hist = defaultdict(list)\n",
    "for r in train_df.sort_values(['user_idx','ts']).itertuples(index=False):\n",
    "    user_hist[int(r.user_idx)].append(int(r.item_idx))\n",
    "# Fix: use cfg.rqvae_levels to match the RQ-VAE configuration (3 levels)\n",
    "vocab_cfg = VocabConfig(codebook_size=cfg.rqvae_codebook_size, levels=cfg.rqvae_levels, user_vocab_hash=cfg.user_vocab_hash)\n",
    "seq_cfg = Seq2SeqConfig(d_model=cfg.seq2seq_d_model, ff=cfg.seq2seq_ff, heads=cfg.seq2seq_heads, layers_enc=cfg.seq2seq_layers_enc, layers_dec=cfg.seq2seq_layers_dec, dropout=cfg.seq2seq_dropout, max_hist_len=cfg.max_hist_len, batch_size=cfg.seq2seq_batch_size, lr=cfg.seq2seq_lr)\n",
    "train_ds = TIGERSeqDataset(user_hist, sid, user_hash_size=vocab_cfg.user_vocab_hash, codebook_size=vocab_cfg.codebook_size, max_hist_len=seq_cfg.max_hist_len)\n",
    "len(train_ds), train_ds[0][1][:8], train_ds[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq model & training (compact)\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tiger_semantic_id_amazon_beauty.src.seq2seq import TinyTransformer, collate_batch\n",
    "V = 1 + vocab_cfg.semantic_vocab + vocab_cfg.user_vocab_hash + 2  # PAD=0, BOS=1, then others\n",
    "model = TinyTransformer(vocab_size=V, d_model=seq_cfg.d_model, ff=seq_cfg.ff, heads=seq_cfg.heads, layers_enc=seq_cfg.layers_enc, layers_dec=seq_cfg.layers_dec, dropout=seq_cfg.dropout)\n",
    "model = model.cuda() if torch.cuda.is_available() else model\n",
    "opt = torch.optim.Adam(model.parameters(), lr=seq_cfg.lr)\n",
    "loader = DataLoader(train_ds, batch_size=seq_cfg.batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "steps = 0\n",
    "for src, tgt in loader:\n",
    "    if torch.cuda.is_available(): src, tgt = src.cuda(), tgt.cuda()\n",
    "    logits = model(src, tgt[:, :-1])\n",
    "    loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), tgt[:, 1:].reshape(-1), ignore_index=0)\n",
    "    opt.zero_grad(); loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
    "    steps += 1\n",
    "    if steps % 200 == 0:\n",
    "        print('steps', steps, 'loss', float(loss))\n",
    "    if steps >= 1000: break  # knob for Colab runtime\n",
    "torch.save(model.state_dict(), f\"{Paths.artifacts_dir}/seq2seq.pt\")\n",
    "steps\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}