{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data EDA - Amazon Beauty Dataset\n",
    "\n",
    "Investigating why all item texts are identical in the TIGER SemanticID pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo, install dependencies, and make src importable (Colab-friendly)\n",
    "try:\n",
    "    import google.colab  # type: ignore\n",
    "    IN_COLAB = True\n",
    "except Exception:\n",
    "    IN_COLAB = False\n",
    "\n",
    "repo_url = 'https://github.com/allyoushawn/recsys_playground.git'\n",
    "repo_dir = 'recsys_playground'\n",
    "branch_name = '20250908_tiger_dev'\n",
    "\n",
    "import os\n",
    "if IN_COLAB:\n",
    "    if not os.path.exists(repo_dir):\n",
    "        !git clone $repo_url\n",
    "    %cd $repo_dir\n",
    "    !git fetch --all\n",
    "    !git checkout $branch_name || echo 'Branch not found; staying on default.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime & setup - same as main notebook\n",
    "import os, sys, subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Make src importable - same path structure as main notebook\n",
    "src_path = os.path.abspath('tiger_semantic_id_amazon_beauty/src')\n",
    "if src_path not in sys.path: \n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import utilities first to setup paths and ensure directories exist\n",
    "from tiger_semantic_id_amazon_beauty.src.utils import set_seed, ensure_dirs, Paths\n",
    "set_seed(42)\n",
    "ensure_dirs(Paths.data_dir, Paths.artifacts_dir)\n",
    "\n",
    "print(f'Data directory: {Paths.data_dir}')\n",
    "print(f'Artifacts directory: {Paths.artifacts_dir}')\n",
    "\n",
    "# Now import data processing functions\n",
    "from tiger_semantic_id_amazon_beauty.src.data import load_reviews_df, load_meta_df, filter_and_split, build_id_maps, apply_id_maps\n",
    "from tiger_semantic_id_amazon_beauty.src.embeddings import build_item_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check raw downloaded files before processing\n",
    "import gzip\n",
    "import json\n",
    "\n",
    "print(\"=== RAW FILE INSPECTION ===\")\n",
    "\n",
    "# Check if files exist\n",
    "reviews_path = f\"{Paths.data_dir}/reviews_Beauty_5.json.gz\"\n",
    "meta_path = f\"{Paths.data_dir}/meta_Beauty.json.gz\"\n",
    "\n",
    "print(f\"Reviews file exists: {os.path.exists(reviews_path)}\")\n",
    "print(f\"Meta file exists: {os.path.exists(meta_path)}\")\n",
    "\n",
    "if os.path.exists(meta_path):\n",
    "    print(f\"\\nMeta file size: {os.path.getsize(meta_path)} bytes\")\n",
    "    \n",
    "    # Read first few lines of meta file to see what columns are available\n",
    "    print(\"First 3 lines of raw meta file:\")\n",
    "    with gzip.open(meta_path, 'rt') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 3:\n",
    "                break\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                print(f\"Line {i+1}: {list(data.keys())}\")\n",
    "                if i == 0:  # Show first record in detail\n",
    "                    print(f\"  Sample data: {data}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Line {i+1}: JSON decode error: {e}\")\n",
    "                print(f\"  Raw line: {repr(line[:100])}...\")\n",
    "                \n",
    "    # Count total lines\n",
    "    print(f\"\\nCounting total lines in meta file...\")\n",
    "    with gzip.open(meta_path, 'rt') as f:\n",
    "        line_count = sum(1 for _ in f)\n",
    "    print(f\"Total lines in meta file: {line_count}\")\n",
    "else:\n",
    "    print(\"Meta file not found - need to download first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix JSON parsing issue - file contains Python dict format, not JSON format\n",
    "import gzip\n",
    "import ast  # Use ast.literal_eval to parse Python dict format\n",
    "\n",
    "print(\"=== PARSING WITH ast.literal_eval ===\")\n",
    "\n",
    "meta_path = f\"{Paths.data_dir}/meta_Beauty_5.json.gz\"\n",
    "if not os.path.exists(meta_path):\n",
    "    meta_path = f\"{Paths.data_dir}/meta_Beauty.json.gz\"\n",
    "\n",
    "# Parse first few lines using ast.literal_eval instead of json.loads\n",
    "print(\"First 3 records using ast.literal_eval:\")\n",
    "with gzip.open(meta_path, 'rt') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 3:\n",
    "            break\n",
    "        try:\n",
    "            # Use ast.literal_eval to safely parse Python dict strings\n",
    "            data = ast.literal_eval(line.strip())\n",
    "            print(f\"\\\\nRecord {i+1}:\")\n",
    "            print(f\"  Keys: {list(data.keys())}\")\n",
    "            print(f\"  asin: {data.get('asin', 'N/A')}\")\n",
    "            print(f\"  title: {data.get('title', 'N/A')}\")\n",
    "            print(f\"  description: {data.get('description', 'N/A')[:100]}...\")\n",
    "            print(f\"  brand: {data.get('brand', 'N/A')}\")\n",
    "            print(f\"  categories: {data.get('categories', 'N/A')}\")\n",
    "        except (ValueError, SyntaxError) as e:\n",
    "            print(f\"Record {i+1}: Parse error: {e}\")\n",
    "            print(f\"  Raw line: {repr(line[:100])}...\")\n",
    "\n",
    "print(\"\\\\n=== CREATING FIXED PARSER FUNCTION ===\")\n",
    "\n",
    "def _parse_python_dict_lines(path: str):\n",
    "    \"\"\"Parse Python dict lines (not JSON) from a gzipped file.\"\"\"\n",
    "    import ast\n",
    "    \n",
    "    opener = gzip.open if path.endswith(\".gz\") else open\n",
    "    rows = []\n",
    "    with opener(path, \"rt\") as f:\n",
    "        for line_num, raw in enumerate(f):\n",
    "            try:\n",
    "                line = raw.strip()\n",
    "                if line:\n",
    "                    # Use ast.literal_eval to parse Python dict strings safely\n",
    "                    data = ast.literal_eval(line)\n",
    "                    rows.append(data)\n",
    "            except (ValueError, SyntaxError, MemoryError) as e:\n",
    "                # Skip malformed lines\n",
    "                continue\n",
    "                \n",
    "    return rows\n",
    "\n",
    "# Test the fixed parser\n",
    "print(\"Testing fixed parser on first 10 records...\")\n",
    "sample_data = _parse_python_dict_lines(meta_path)\n",
    "print(f\"Successfully parsed {len(sample_data)} records\")\n",
    "\n",
    "if sample_data:\n",
    "    print(f\"\\\\nSample record keys: {list(sample_data[0].keys())}\")\n",
    "    print(f\"Sample record: {sample_data[0]}\")\n",
    "\n",
    "# Apply the fix to the data.py functions\n",
    "print(\"\\\\n=== PATCHING DATA LOADING FUNCTIONS ===\")\n",
    "from tiger_semantic_id_amazon_beauty.src import data\n",
    "\n",
    "# Replace the broken JSON parser with our fixed parser\n",
    "data._parse_json_lines = _parse_python_dict_lines\n",
    "print(\"âœ“ Patched data._parse_json_lines with Python dict parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data with FIXED parser\n",
    "print(\"=== LOADING RAW DATA WITH FIXED PARSER ===\")\n",
    "reviews = load_reviews_df(f\"{Paths.data_dir}/reviews_Beauty_5.json.gz\")\n",
    "meta = load_meta_df(f\"{Paths.data_dir}/meta_Beauty.json.gz\")\n",
    "\n",
    "print(f\"Reviews shape: {reviews.shape}\")\n",
    "print(f\"Meta shape: {meta.shape}\")\n",
    "print(f\"Reviews columns: {reviews.columns.tolist()}\")\n",
    "print(f\"Meta columns: {meta.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\\\nMeta data sample (first 3 rows):\")\n",
    "print(meta.head(3))\n",
    "\n",
    "print(f\"\\\\nMeta data dtypes:\")\n",
    "print(meta.dtypes)\n",
    "\n",
    "print(f\"\\\\nNull values in meta:\")\n",
    "print(meta.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check specific columns that should contain item information - FIXED for list columns\nprint(\"=== CHECKING KEY COLUMNS ===\")\nkey_cols = ['title', 'description', 'category', 'categories', 'brand', 'price']\n\ndef safe_analyze_column(df, col):\n    \"\"\"Safely analyze a column that might contain lists or other unhashable types.\"\"\"\n    if col not in df.columns:\n        print(f\"\\n{col} column: NOT FOUND\")\n        return\n        \n    print(f\"\\n{col} column:\")\n    \n    # Check if column contains lists by examining a sample value\n    sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n    is_list_column = isinstance(sample_val, list)\n    \n    if is_list_column:\n        # Handle list columns specially (can't use nunique on lists)\n        non_null_count = df[col].dropna().shape[0]\n        print(f\"  Non-null values: {non_null_count} (contains lists)\")\n        print(f\"  Null values: {df[col].isnull().sum()}\")\n        print(f\"  Sample values: {df[col].dropna().head(3).tolist()}\")\n    else:\n        # Handle regular columns normally\n        try:\n            print(f\"  Unique values: {df[col].nunique()}\")\n            print(f\"  Null values: {df[col].isnull().sum()}\")\n            print(f\"  Sample values: {df[col].dropna().head(3).tolist()}\")\n        except Exception as e:\n            print(f\"  Analysis error: {e}\")\n            print(f\"  Sample values: {df[col].dropna().head(3).tolist()}\")\n\nfor col in key_cols:\n    safe_analyze_column(meta, col)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data as in main notebook\n",
    "print(\"=== DATA PROCESSING ===\")\n",
    "from tiger_semantic_id_amazon_beauty.src.data import BeautyConfig\n",
    "train_df, val_df, test_df = filter_and_split(reviews, BeautyConfig())\n",
    "user2id, item2id = build_id_maps([train_df, val_df, test_df])\n",
    "train_df = apply_id_maps(train_df, user2id, item2id)\n",
    "val_df = apply_id_maps(val_df, user2id, item2id)\n",
    "test_df = apply_id_maps(test_df, user2id, item2id)\n",
    "\n",
    "print(f\"Number of unique items in item2id: {len(item2id)}\")\n",
    "print(f\"Sample item_ids: {list(item2id.keys())[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create items dataframe as in main notebook\n",
    "print(\"=== ITEMS DATAFRAME CREATION ===\")\n",
    "items = pd.DataFrame({\n",
    "    'item_id': list(item2id.keys()), \n",
    "    'item_idx': list(item2id.values())\n",
    "}).merge(meta, on='item_id', how='left')\n",
    "\n",
    "print(f\"Items shape: {items.shape}\")\n",
    "print(f\"Items columns: {items.columns.tolist()}\")\n",
    "print(f\"\\nFirst 5 items:\")\n",
    "print(items.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if items have diverse metadata - FIXED for list columns\nprint(\"=== ITEMS DIVERSITY CHECK ===\")\n\ndef safe_analyze_column(df, col):\n    \"\"\"Safely analyze a column that might contain lists or other unhashable types.\"\"\"\n    if col not in df.columns:\n        print(f\"{col}: Column not found\")\n        return\n        \n    # Check if column contains lists\n    sample_val = df[col].dropna().iloc[0] if not df[col].dropna().empty else None\n    is_list_column = isinstance(sample_val, list)\n    \n    if is_list_column:\n        # Handle list columns specially\n        non_null_count = df[col].dropna().shape[0]\n        total_vals = len(df)\n        print(f\"{col}: {non_null_count} non-null values out of {total_vals} items (contains lists)\")\n        print(f\"  Sample values (first 3):\")\n        for i, val in enumerate(df[col].dropna().head(3)):\n            print(f\"    {i+1}: {val}\")\n        \n        # Count unique list lengths\n        if non_null_count > 0:\n            lengths = df[col].dropna().apply(lambda x: len(x) if isinstance(x, list) else 0)\n            print(f\"  List lengths - min: {lengths.min()}, max: {lengths.max()}, mean: {lengths.mean():.1f}\")\n    else:\n        # Handle regular columns\n        try:\n            unique_vals = df[col].nunique(dropna=True)\n            total_vals = len(df)\n            print(f\"{col}: {unique_vals} unique values out of {total_vals} items\")\n            \n            if unique_vals <= 10:  # Show actual values if small number\n                print(f\"  Values: {df[col].dropna().unique().tolist()}\")\n            else:\n                print(f\"  Sample: {df[col].dropna().head(5).tolist()}\")\n        except Exception as e:\n            print(f\"{col}: Analysis error - {e}\")\n            print(f\"  Sample: {df[col].dropna().head(3).tolist()}\")\n\nfor col in ['title', 'description', 'category', 'categories', 'brand']:\n    safe_analyze_column(items, col)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the build_item_text function\n",
    "print(\"=== TESTING build_item_text FUNCTION ===\")\n",
    "# Take first 10 items\n",
    "sample_items = items.head(10)\n",
    "texts = build_item_text(sample_items)\n",
    "\n",
    "print(f\"Generated {len(texts)} texts\")\n",
    "print(f\"All texts identical? {all(texts[0] == text for text in texts)}\")\n",
    "print(f\"\\nFirst 3 generated texts:\")\n",
    "for i, text in enumerate(texts[:3]):\n",
    "    print(f\"Text {i}: {repr(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the build_item_text function implementation\n",
    "print(\"=== EXAMINING build_item_text IMPLEMENTATION ===\")\n",
    "import inspect\n",
    "print(\"Function source:\")\n",
    "print(inspect.getsource(build_item_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual text building to debug\n",
    "print(\"=== MANUAL TEXT BUILDING DEBUG ===\")\n",
    "sample_items = items.head(5)\n",
    "print(\"Raw item data:\")\n",
    "for i, (_, item) in enumerate(sample_items.iterrows()):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    for col in ['item_id', 'title', 'description', 'category', 'categories', 'brand']:\n",
    "        if col in item:\n",
    "            print(f\"  {col}: {repr(item[col])}\")\n",
    "        else:\n",
    "            print(f\"  {col}: NOT FOUND\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}